## 딥러닝 

ChatGPT와 Diffusion을 제대로 이해하려면 이들의 기반인 딥 러닝 네트워크를 이해해야함  

### 딥 러닝 프레임워크 

1. 딥러닝의 정의와 활용 가능 분야, 이를 지원하는 라이브러리(텐서플로, 파이토치)  

2. 기본 구성요소인 퍼셉트론과 은닉층, 다층 퍼셉트론, 순전파, 역전파  

3. 딥러닝 모델을 구현할 떄 주의사항, 모델의 성능을 높이기 위한 방법

ex) 과적합 방지, 하이퍼 파라미터 설정, 모델성능 최적화  ==> 모델 일반화 능력 향상 

딥러닝의 핵심원리를 쉽게 이해하자   

## 01. 딥 러닝이란?  

인공지능의 학습방법 중 하나로 인간의 뇌와 신경계를 모방해서 만든 **인공 신경망**이라는 구조를 통해 학습  

**신경계** 

몸 안팎에서 일어나는 변화로 인한 자극을 빠르게 전달해서 그에 대한 반응을 생성하는 기관들 

신경계의 기본 단위 : 뉴런  

뉴런은 앞부분에서 정보를 받아서 처리한 후 뒤로 정보를 전달함  

신경계에는 이런 뉴런들이 100조개 이상의 시냅스를 통해 연결되어 있음  


![](/image/1-1.PNG)

딥러닝도 이와 유사하게 입력데이터를 받아 인공뉴런을 통해 출력값으로 전달  


![](/image/1-2.PNG)  



**인간의 신경계**에서 수많은 **뉴런**들이 연결되어있는 것처럼,   

 **딥러닝 모델**에서는 **인공뉴런**들이 수많은 연속된 층으로 구성되어 있음  

이런 층이 엄청 깊다고 해서 **딥러닝**이라는 이름이 붙음  

![](/image/1-3.PNG)  


**딥러닝의 장점**  
1. 다양한 입력 데이터 처리 
2. 상대적으로 높은 성능 달성  

**딥러닝의 단점** 
1.  훈련시 많은 양의 데이터 필요
2.  모델의 결정이 대한 해석이 어려움 

## 딥러닝의 다양한 활용 분야   

의료, 금융, 농업, 제조 등 다양한 분야에서 기술적 진보를 이끔  

의료 분야 : 1. 의료 영상 분석(피부암, 눈 망막 이미지 분석) 2. 유전 데이터 분석 3. 질병의 진단 및 예측 4. 신약 개발  

금융 분야 : 1. 주식시장의 변동 예측 2. 고객 신용 평가 3. 사기 탐지 (비정상적인 거래패턴)  

농업 분야: 1. 농작물 관리 2. 수확량 예측 3. 병해충 감지  

제조 분야: 1. 생산 과정 자동화 2. 품질 관리  


크게 3가지로 분류가능 1. 컴퓨터 비전 2. 자연어 처리 3. 강화학습  

## 컴퓨터 비전 
- 이미지 분류
- 객체 탐지
- 얼굴 인식
- 이미지 생성 
- 자율주행 차량( 딥러닝을 통해 주변 상황을 인식 의사결정을 내려서 장애물을 피함 )
## 자연어 처리
- 텍스트 번역
- 감정 분석
- 챗봇 개발
- 음성 인식 
  ex) 챗gpt  

## 강화학습 
- 복잡한 환경에서의 최적의 결정
- 여러 번의 시행착오롤 통해 학습 
  ex) 알파고 

## 딥러닝 프레임 워크 소개 

딥 러닝 프레임워크 : 딥 러닝 모델을 설계, 훈련, 검증하기 위해 사용되는 **소프트웨어 라이브러리**나 **도구** 모음  

**필요성**
 1. 복잡한 수치계산과 미분을 자동화하여 효율성 증대
=> 모델설계와 실험에 더 많은 시간 할애  

2. 모델 컴포넌트를 모듈화하여 쉽게 재사용 가능 
- 모듈 : 딥러닝 요소 하나하나  

- 모듈화: 일련의 딥러닝 과정을 요소로 나눠서 효율적으로 관리  

3. 소규모 데이터셋에서 대규모 데이터셋으로 확장을 용이하게 함 

(파이토치, 텐서플로 각각의 사용법은 차후에 소개)

## 파이토치( PyTorch)
- Meta에서 개발 
- 인터페이스가 직관적이라 사용에 용이
- 파이썬스럽다 => 파이썬 사용자면 어렵지 않게 배울 수 있음 

## 텐서플로 (TensorFlow)
- Google에서 개발 
- 확장성이 높아서 다양한 기기와 플랫폼에서 사용가능
- 대규모 프로젝트 배포에 용이
- 복잡하고 어려움 => 2버전이 나오면서 **keras** 사용하면서 편해짐  

파이토치와 텐서플로를 활용하면 복잡한 딥러닝 모델을 효율적으로 설계하고 학습시킬 수 있음  

## 02. 딥러닝 기초 이론  

## 퍼셉트론(Perceptron)

인공 신경망의 가장 기본적인 형태, 인공뉴런이라고도 불림  

노드: 신경망에서 정보를 처리하는 기본 단위, 신경망의 뉴런에 해당함 

각 노드는 여러 입력값을 받아 가중치를 곱한후 이를 합산하여 출력값을 생성  
초기 퍼셉트론은 이진분류에 사용됨

![](/image/1-5.PNG)  


이 과정에서는 **계단함수**가 사용됨   
**계단함수** :  입력값에 따라 임계값을 기준으로 1또는 0이라는 출력값을 내보내는 역할  

![](/image/1-6.PNG)  


수식으로 표현하면?  

![](/image/1-7.PNG)  


여기서 b는 편향(bias)라고 불리고 퍼셉트론의 각 노드에 추가되어 있는 항을 말함  


입력값을 보내는 단계와 값을 받아서 출력하는 두 단계로만 이루어진 퍼셉트론을 **단층 퍼셉트론**이리함  

이 두 단계는 각각 층이라고 불리며 신경망에서는 입력층과 출력층이라고 부름   


![](/image/1-8.PNG)  


입력층은 외부에서 데이터를 받아들이는 역할  
 출력층은 처리된 데이터를 바탕으로 최종 결과를 생성하여 외부로 전달하는 역할  


출력층을 구성하는 노드의 개수는 1개일 수도 여러 개일 수도 있음  


딥러닝 모델에서 출력노드의 개수는 모델이 해결하고자 하는 문제의 형태에 따라 결졍됨  


  
![](/image/1-9.PNG)  

**단층**은 **간단한 패턴처리**에는 유용 **복잡한 패턴**은 **다층** 퍼셉트론이 필요  

## 은닉층과 순전파   

단충 퍼셉트론은 선형경계를 설정하는데 특화된 모델    

![](/image/1-10.PNG)  

그러나 대부분의 실생활 데이터는 선형으로 구분이 어려움 => 복잡한 모델링이 요구됨  

이러한 한계를 극복하기 위해 **다층 퍼셉트론**이 등장  


**다층 퍼센트론(Multi-Layer Perceptron)**  

**MLP**라도고 함  

![](/image/1-11.PNG)  

여기서 **레이어(Layer)**라는 개념이 등장  


여러 노드들이 모인 집합  

신경망의 구조를 구성하는 단위  

신경망은 일반적으로 입력층, 은닉층, 출력층으로 구성  

![](/image/1-12.PNG)  

은닉층이 없으면 단층 퍼셉트론  

하나 이상이면 다층 퍼센트론이라 부름  

은닉층은 필요에 따라 여러개를 추가할 수 있으며  여러개를 가진 깊은 구조의 신경망을 **심층 신경망 (Deep Neural Network)** 이라고 함   

이런 다층 퍼셉트론을 활용하면 좀 더 복잡한 결정경계를 형성할 수 있음  


**은닉층**  

- 입력층과 출력층 사이에 위치
- 네트워크의 입력과 출력사이에서 복잡한 특성이나 패턴을 추출하고 학습하는 역할 
- 비선형적 데이터의 특성 학습 가능 -> 보다 정교한 모델 구축 가능!   

다층 퍼셉트론에서는 각 층에서 수행되는 가중합 연산을 **순전파**라는 방식으로 진행  

**순전파**란 신경망에서 입력데이터가 네트워크의 각 층을 순차적으로 통과하여 최종출력을 생성하는 과정   


이 과정에서 각 층의 뉴런은 **활성화 함수**를 사용하여 다음 층으로의 출력을 생성함  

![](/image/1-13.PNG)  


결과적으로 생성된 출력값은 예측값으로 사용되며 이 예측값을 실제값과 비교하는 손실함수를 통해 네트워크의 성능을 평가하고 최적화함  


**손실함수(Loss Function)**는 모델의 예측이 실제데이터와 얼마나 차이나는지를 측정하고 이 정보를 바탕으로 모델의 학습을 조정  


다층 퍼셉트론에서는 은닉층을 여러 개 쌓는 것 뿐만 아니라 시그모이드 같은 비선형 비활성화 함수를 사용함  

사실 은닉층을 여러개 쌓아도 활성화 함수가 선형이면 비선형 연산은 불가능함  

그래서 각 층마다 비선혀 활성화 함수를 적용해서 좀 더 복잡한 데이터를 효과저긍로 학습할 수 있도록 함   

시그모이드 이외에 다양한 비선형 활성화 함수가 연구되었음 (tanh, ReLU, Leaky ReLU)
이들은 각기 다른 특징을 가지고 신경망의 성능을 개선하는데 기여함 

## 역전파(Backpropagation)

지난 레슨에서는 순전파 과정을 통해 생성도니 예측값과 실제 정답과의 차이를 손실함수를 사용해서 나타내는 방법을 배움
1-15

그렇다면 순전파가 잘된, 즉 딥러닝 모델이 잘 학습되었다게  무슨말일까?  
최종 출력값 (예측값)과 실제값에 가가워지며 오차가 최소화된 상태 

우리의 목표는 손실함수의 값을 가능한 한 최소한으로 줄이는 것  

그러나 초기 가중치가 랜ㄷ머하게 설정되기 떄문에 처음에는 실제값과 예측값사이에 상당한 오차가 발생함 
이 오차를 줄이기 위해 필요한 과정이 바로  **역전파**  

역전파는 출력에서 발생한 오차를 거슬러 올라가며 각 층의 가중치가 오차에 얼마나 기여하는지를 계사하고 이 정보를 사용하여 가중치를 업데이트함  
이과정을 통해 신경망은 오차를 최소화하는 방향으로 점차 개선 됨 

역전파과정에서는 경사하강법을 통해 손실함수를 최소화하는 방향으로 가중치 조정이 이루어짐 

경사하강법은 손실함수의 기울기를 계싼하고 그래디언트의 반대방향으로 즉 기울기를 곘나하고 반방향으로 조금씩 이동하면서 함수의 최솟값을 찾아가는 최적화 기법 

1-17  

## 경사하강법 과정 구체저긍로 

1. **초기 가증치** 랜덤하게 설정 
2. 이 초기상태에서 손실함수의 최솟값을 향해 어떻게 움직여야할지 결정하는게 이때 함수의 기울기를 계산하여 기울기의 반대방햐응로 가중치를 움직임 ( 기울이가 0보다 클떄는 가중치 감소, 그럻지 않을떄는 가중치를 증가시킴)
3. 위 과정 반복 후 최적값 도달  -> 손실을 최소화하는 가중치 값 도출  

수식 

1-18  

수식에서 알파에 해당하는 부분이 **학습률(Learning Rate)** 

## 학습률 
- 각 반복에서 가중치가 얼마나 조정될지 결정  
- 모델이 얼마나 빠르게 최적의 솔루션에 도달할 수 있는지에 영향을 미침 
- 너무 높으면 손실(Loss)값 발산 우려 
- 너무 낮으면 학습 속도 저하 우려 

따라서 적절한 학습률을 설정하는 것은 모델의 학습효율 과 성능을 극대화 하는 데 매우 중요  


## 출력함수  

신겨망의 마지막 층에 위치, 신겸아응ㄹ 통해 계싼된 값을 우리가 원하는 최정결과 형태로 변환해줌   예를 들어 회귀나 분류와 같은 무넺를 해결할떄 출력함수가필=수적으로 사용됨

각각의 겨우에 대해 설명 

1. 회귀 문제에서의 출력 함수  
보통 신경망의 마지막 출력값을 그대로 사용 
y = wx + b

  예를 들어 주택가격을 예측하는 회귀모델이 2억이라는 출력값을 도출했다면 
  모델이 예측한 주택가격은 2억원

2. 분류 문제에서의 출력 함수  

분류 문제에서는 조금 다룬 형태의 함수가 필요  
이진 분류:  **시그모이드 함수**를 이용해 출력갑승ㄹ 0과 1사이로 변환 
출력값은 해당 클래스에 속할 확률로 해석 

예시) 스팸 메일 예측에서 출력값이 0.85라면 85% 확률로 스팸 메일이라고 예측 
보통 기준은 0.5러ㅗ 잡읍 

다중 분류 문제에서의 출력 함수  
출력함수로 소프트 맥스 함수를 사용함 
소프트 맥스 함수는 각 클래스에 대한 출력을 확률분포로 변환하여 각 클래스에 속할 확률을 나타냄 
0과1사이의 값이 나옴 
1-19
1-20
여기서 Zi는 모델의 i클래스에 대한 입력 x와 가중치 w의 선형조합

이번에는 사진에 나타난 동물이 고양이 개 새 중 어떤 것인지 분류하는 문제가 잇다고 하자 

출력노드는 클래스의 개수만큼 3개가 있음 
여기서 소프트맥스 함수를 하용하여 각 클래스에 대한 확률분포를 출력함  

각 출력값은 해당클래스에 속할 확률로 해석되며 모든 클래스의 확률 합은 1

1-21  

이렇게 딥러닝 모델은 다양한 문제 상황에 맞게 출력함수를 선택하여 적절한 결과를 도출 할 수 있음 
이 과정을통해  신경망은 특정 입력에 대해 가장 적절한 출력을 제공함 
 
## 손실 함수(Loss Function)
모델이 실제값을 얼마나 잘 예측하는 지를 측정하는 함수  

회귀문제인지 분류 문제인지에 따라 사용하는 손실함수도 달라짐  

먼저 회귀문제에서는 평균 제곱 오차(Mean Squared Error, MSE)를 사용 
(MSE는 머신러닝 로드맵에서 공부해서 넘어감)  

분류 문제엥서의 손실 함수
이진 분류 : 이진 교차 엔트로피 (Binary Cross-Entropy)
다중 분로 : 교차 엔트로피 (Cross-Entropy)

이 두 유형의 손실함수는 모델이 불확실성을 가질 떄 더 큰 손실을 부과함  
이를 통해 모델이 더 정확한 예측을 하도록 유도  


## 이진 교차 엔트로피 (Binary Cross-Entorpy)  

모델이 예측한 확률이 실제값과 얼마나 가까운지를 측정  

수식 
1-22  

## 교차 엔트로피(Cross-Entropy)  

모델은 여러 클래스중 하나에 속할 확률을 예측하고 모델이 예측한 확률분포가 실제레이블의 분포와 얼마나 일치하는지를 평가함  

수식  

1-23  

결과적으로 모델이 불확실성을 덜 가지고 정확한 예측을 하도록 도움 


## 최적화가 어려운 이유

딥러닝모델을 최적화 하는 과정은 특히 어려움 
여러 복잡한 요소의 영향을 받음
이런 원인은 1. 모델의 비선형성 2. 고차원성과 과적합 3. 그래디언트 소실 문제 4. 하이퍼 파라미터의 민감성 이라는 4가리조 나누어 살펴보자 

1. 모델의 비선형성 
딥러닝은 비선형 활성화 함수를 도입하여 모델이 더 복잡한 패턴을 학습할 수 있또록 했음
모델에 더 많은 표현력을 부여하지만 손실함수의 표면을 복합하게 만들어 전역 최솟값(Global Minimum)을 찾기 어렵게 함  

2. 고차원성과 과적합 
- 현재 딥 러닝 모델은 매우 많은 개수(수십만~ 수십억개)의 파라미터(가중치, 편향)를 포함 
ex) GPT3 : 1750억개의 파라미터 사용 
- 과적합 위험 증가-> 이를 방지하기 위해 데이터 증강, 드롭아웃, 정규화 기법등을 사용가능 

3. 그래디언트 소실 (Gradient Vanishing)
- 네트워크가 깊어질수록 많은 개수의 레이어플 거치면서 발생
- 가중치에 대한 손실함수의 미분값이 점점 작아져서 초기 층의 가중치 업데이트가 거의 이루어지지 않게 되는 현상  
- 학습이 진행될수록 그래디언트가 소실되어 학습이 제대로 이루어지지 않음
- 그래디언트 소실이 발생하는 주된 이유는 신경망에서 사용하는 활성화 함수 댸문인데 시그모디으 함수는 출력감수의 범위가 0~1로 제한되어 있음
이 함수를 계속해서 미분하면 그래디언트의 값이 점점 작아짐 

1-24 

신경망이 깊어질수록 이런현상은 더욱 누적되고 결국에는 초기층에 가중치들이 거의 업데이트 되지 않음 
이런 문제를 해결하기 위해 그레디언트 소실에 덜 민감한 활성화 함수를 사용하거나 
적절한 가중치 초기화 방법을 사용할 수 있고 배치정규화라는 방법도 사용될 수 있음  

4. 하이퍼 파라미터의 민감성 

하이퍼 파라미터는 모델의 학습률, 배치 크기, 층의 수 등과 같이 모델을 학습하기 전에 설정되는 변수
- 이들을 적절하게 조정하는 것은 최적의 모델 성능을 달성하기 위해 매우 중요  
-  적절한 하이퍼 파라미터를 찾는 과정은 시행착오와 많은 실험을 필요로 함 
떄로는 자동화하는 방법을 사용하여 과정을 효울화함  

## 딥 러닝 모델링 시 고려할 사항  

하이퍼 파라미터의 종류 
1. 배치크기 (Batch Size) 
- 한 번의 학습 단계에 사용되는 데이터 샘플의 수
- 메모리 사용량과 학습 속도에 큰 영향 


2. 학습률 (Learning Rate)
- 모델의 가중치를 어느 정도로 업데이트 할지 결정하는 값 
- 역전파 과정에서 계산된 그래디언트에 학습률을ㄱ보해 가중치를 업데이터  
1-25 
1-26

3. 에폭 수 (Epoch) 
- 전체 훈련 데이터셋이 네트워크를 통과하는 횟수 즉, 모든 훈련데이터가 모델을 통해 한 번 학습되는 것이 하나의 에폭 
- 너무 많으면 학습 시간이 길어지고, 과적합을 초래 

4. 옵티마이저(Optimizer)  
- 손실 함수를 최소화하기 위해 모델의 가중치를 어떻게 업데이트할지 결정 
ex) 확률적 경사 하강범(SGD), 모멘텀(Momenutm), 아담(Adam)등 

## 다양한 활성화 함수 알아보기


**활성화 함수 (Activation Function)**

- 신경망의 각 뉴런에서 입력 신호의 총합을 받아 이를 출력신호로 변환하는 역할 
- 주로 비선형 형태로, 신경망이 복잡한 문제를 해결하는데 기여
- 다양한 패턴과 관계를 효과적으로 모델링 

가중합 연산을 통해 구한값을 비선형 활성화함수를 통해 변형해주고 이를 최종신경망에 출력값으로 출력하는 형태  

1-27  

각각의 함수는 특정상황에서 장점을 가짐 
주로 사요오디는 함수 몇가지와 그 특성에 대해 알아보자  
 

1. 시그모이드 함수 

1-28

2. Tanh (Hyperbolic Tangent, 쌍곡) 함수 

1-29 

3. ReLU(Rectified Linear Unit) 함수 
1-30

- 입력이 양수일 때는 그래디언트가 1이 되므로 그래디언트가 감소하지 않음 
이는 깊은 신경망에서도 정보와 그래디언트가 층을 통과하면서 사라지지 ㅇ낳고 역전파동안 효과적으로 전달됨 결과적으로 깊은 층의 가중치가 활성화 상태에서 지속적으로 업데이트 될 수 있음 
- 입력이 음수일 떄 그래디언트도 0이 됨 뉴런이 완전히 비활성화됨 
'죽은 뉴런'문제로 알려져 있고 ㅠㄴ런이 앗앹애ㅔ 이르면 학습과정중에 해당 뉴런이 다시 활성화 되거나 학습에 기옇나ㅡㄴ 것이 어려워짐. 실질적으로 학습에서 제외됨
이문제를 개선하기 위해서  **Leaky ReLU 함수**가 등장

4. Leaky ReLU 함수 
1-31
여기서 a는 0.01같은 작은 값을 주로 사용함 음수입력에서도 아주 작은 기울기를 허용함으로써 모든 입력에 대해 그래디언트를 유지하려고 함 

이러첨 다양한 종류의 활성화 함수가 있는데 이러한 함수우ㅢ 특성을 이해하기 주어진 문제에 가장 적합한 함수를 선택하는 것이 중요 

## 다양한 경사 하강법 알아보기  

배치 라는 개념 복습  
**배치(Batch)** : 한 번의 학습 단계(iteration)에서 사용되는 학습 데이터의 묶음  
모든 배치셋들  전체 데이터를 학습하면 1에폭을 학습한 것이 됨  
1-32 

1. 배치 경사 하강법 (Batch Gradient Descent)
전체 학습 데이터셋에 대한 에러를 구한뒤 기울기를 한 번만 계산하여 모델의 파라미터를 없데이트 하는 방식 

1-33

한 번에 모든 데이터를 고려하여 가중치를 업데이트 
그러나 이 방법은 대귬도 데이서텟에서는 계산비용이 매애 눞고 빠르게 수렴하는 것이 어려움 
이런 비효율서응ㄹ  개선하기 위해 확률적 경사 하강법 등장
2. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)
확률적 경사 하강법은 한번의 이터레인션에 무작위로 서택된 하나의 데이터 ㅅ매플을 사용하여 그래딩더느를 꼐산하고 기울기를 계산 

1-34

기본적이지만 많은 문제에서 효과적일 수 있음 
무작위성 떄ㅁ눈에 지역최솟값에서 벗어나전역 최솟값을 찾을 가능성이 있음
계산 속도가 빠름 . 경사의 추정이 불안정하여 학습과정이 매끄럽지 않을 수 있음 \
3
절충안 미니배치 경사하강법 (MINIBATCH GRADEITN DESCENT)

최신 딥러닝 기술에서 가장 많이 사용되는 기법 
기본 아이디어는 전체 데이터셋을 미니배치라는 그룹들로 나누고 각 미니배치에 대해 독립저긍로 경사를 계산한후 모델으 ㅣ매개변수를 ㄹ업데이트 
1-35 

이 과정은 전체 데이터 셋이 모두 처리될ㄷ)까지 반복됨 
예시) 1000개의 학습데이터에서 배치 사이즈를 100으로 잡았으면 총 10개의 미니배치가 나옴 그러면  1에 폭당 총 10번의 경사 하강법을 진행 
높은 메모리 효율성, 빠른 계산 속도, 
미니배치의 무작위 선택은 지역 최솟값에 갇히는 것을 방지 
배치 ㅡ키과 학습률등 하이퍼 파라미터에 매우 민감 => 적절한 설정 필요 
 
 ## 다향한 Optimizer 알아보기  

 초지거화 알고리즘 : 신경망에서 가중치를 업데이트 하는 방법을 의미
 각 알고리즘의 원리를 알아두면 나중에 보다 심화딘 문제에 접글할떄 도움이 됨 
 대부분의 딥러닝 프레임워크는 이러한 최적화 알고리즘을 내장하고 있음 
 사용자가 복잡한 수식을 계산할 필요없이 간단한 호출만으로 사용 가능  

크게 세 가지 카테고리로 나눌 수 있음 
1. Momentum 계열 2. Adagrad 계열 3. Adam 계열  


모멘텀 (Momentum) : **이전**업데이트의 방향을 고려해서 가중치 업데이트를 진행   
이전에 움직였던 스텝 방향을 중심적으로 고려 
수식으로 표현하면 다음과 같음  

2. Adagrad (Adaptive Gradient)
- 다음은 움직이는 스텝 사이즈 즉, 학습률을 조절하는것에 초점을 최적화 방법
매개변수에 대해 학습률을 동적으로 조정하는 최적화 알고리즘 
자주 업데이트되는 매개변수(가중치)의 학습률을 점차 감소 시키고
드물게 업데이트되는 매개변수의 학스뷸ㄹ을 상대적으로 높게 유지하여 
각 매개변수가 적절한 속도로 학습되도록 도움 

- 학습이 지속되며 점점 가중치 갱신량이 0이 되는 문제가 있음 
이를 개선하기 위해 과거의 기울기는 조금만 반영하고 최신으 기울기는 많이 반영하는 **RMSProp**이라는게 등장함  


3. Adam 
현재 가장 대표적으로 사용되는 Optimizer 
스텝방향 + 스템 사이즈를 조절 
모멘텀 기반의 방법과 RMSProp 방법을 절충  

더 다양한 옵티마이저가 있음 
문제의 특성, 모델의 구조, 데이터의 양 등에 띠라 최적의 옵티마이저를 사용하는 것이 좋음  

## Normalization 기법 알아보기 

**정규화(Normalization)** : 데이터의 범위를 일정하게 조정하거나 데이터 분포를 표준화하는 과정 
모델의 학습속도를 빠르게 하고 과적합을 방지하는데 도움이 됨   

이런 정규화과정은 1. 입력대이터에 대해 데이터 전처리 과정에서 적용될 수 있고
2. 신경망 내부에 있는 중간층의 활성화 함수에도 적용될 수 있음  

정구화를 적용하지 않는 상태에서는 모델이 최적화 과정에서 전체데이터를 잘 파악하지못하고 지역 최저점에 갇혀 전역 최저점을 찾지 못하는 경우가 발생할 수 있음 
1-36  

정규화를 적용하면 완화할 수 있음  

지역 최솟점이 쉽게 벗어남 최적화 과정이 더 효율저긍로 진행될 수있또록 도움 

나아가 신경망 내부에서 중간층의 활성화 함수에 입력되는 값을 정규화함으로써 
학습을 더 안정적이고 빠르게 진행되도록함 
특히 입력의 분포가 각 층을 지날 떄마다 일정하게 유지되도록 함 -> 효율적인 학습 촉진 . 이때 **배치 정규화 (Batch Normalization)** 기법이 주로 사용됨 

## 배치 정규화 

네트워크의 각 층에서 활성화 함수가 입력 받는 값을 정규화  
입력값으 평균과 분산의 조정을 통해 이루어지고 이 과정은 학습과정에 직접적으로 포함됨  

예시) 각 배치별로 평균과 분산을 이용해 정규화를 수행하는 별도의 레이어를 배치 

1-37  

~~수식과 내부 공변량 변화 생략~~   


## 배치 정규화의 효과
- 각 레이어를 거치며 발생할 수 잇는 입력값의 변동을 효과적으로 제어  
- 결과적으로 학습 속도를 높이며 더 안정적인 학습이 가능  
- 특히 깊은 신경망에서 그래디언트 소실이나 폭발 문제를 완화하는데 매우 효과적  

## Regularization 기법 알아보기   

규제화 방법 종류는 크게 4가지

1. L1 정규화 (Lasso)와 L2 정규화 (Ridge) (머신러닝에서 공부한 내용) 

1-38  

- 가중치의 절댓값에 비례하는 항을 손실함수에 추가 
- 가중치 중 일부를 0으로 만들어 모델을 단순하게 만듦 

1-39  

- 가중치의 제곱을 손실함수에 추가
- 가중치의 극단적인 값에 페널티를 주어 과적합 방지  

2. 드롭아웃(Dropout)

- 훈련 과정 중 무작위로 일부 뉴런을 비활성화 시킴
- 모델이 특정 뉴런이나 뉴런의 조합에 과도하게 의조ㅎㄴ하느 것을 방지 
예시) 드롭아웃 비율울 0.5로 설멍하면?  랜덤으로 전체 뉴런 중 반을 제외시킴 

1-40 

3. 조기 종료 (Early Stopping)

- 딥러닝 모델을 학습시키다 보면 일정 시간이 지나면 검증세트에 대한 손실값이 오히려 증가하느 양상을 보이는 경우가 많음 
- 검증 세트의 성능이 더 이상 개선되지 않을 때 학습을 조기에 중단 
- 모델이 불필요하게 오래 훈련되어 과적합이 발생하는 것을 방지 
 
1-41  

4. 데이터 증강 (Data Augmentation) 

- 원본데이터에 변형을 가해 인위적으로 데이터의 양을 늘리는 방법 
- 모델이 다양ㅇ한 변형을 경험하게 함으로써 일반화 능력을 강화시킴 
- 특히 이미지 데이터에 효과적
- 회전, 이동, 확대, 축소등의 변형을 적용하여 모델의 일반화 능력을 향상시킴 

1-42

## 데이터 증강 기법 알아보기  

- 데이터 증강 기법은 컴퓨터 비전과 같은 분야에서 모델의 성능을 향상시ㅣㅋ고 과적합을 방지하기 위해 주로 사용되는 기술
- 기존의 훈련데이터를 다양한 방식으로 변형하거나 조작하여 더 많고 다양한 데이터에서 학습한 것처럼 만드는 과정  

**데이터 증강의 필요성**
1. 제한된 데이터 만으로도 다양한 훈련 샘플을 생성해 학습 효율 증가 
2. 데이터 불균형 문제 해결 일부 클래스의 데이터가부족하다면 그 크래스의 데이터를 인위적으로 늘려 모든 클래스이 데잍러ㅡㄹ 공평하게 학습하도록 도움 
3. 실제 환경의 변화를 반영하여 모델을 더욱 견고하게 만듬 빛의변화나 노이즈같은 요소들을 의도적으로 데이터에 추가하여 모델이 다양한 상황에서도 일관된 성능을 보이게 함 

**몇 가지 데이터 증강 기법**  

1. 이미지 회전 (Image Rotation) 

1-43 

2. 이미지 확대/축소 (Image Scaling) 

1-44 

3. 이미지 크롭 (Image Crop) 

1-45 

4. 밝기, 대비, 채도 변경 

1-46

이미지 데이터 말고 다른 데이터들에도 증강 기법을 적용할 수 있음 

텍스트 데이터에 대해 증강을 진행한다면? 유의어 교체, 임의의 단어 삽입, 삭제등이 가능 

